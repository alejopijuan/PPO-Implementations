{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, device(type='cpu'))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from torch.distributions import Categorical\n",
    "from multiprocessing_env import SubprocVecEnv\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "#CUDA\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "use_cuda,device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number on envs we'll be running in parallel\n",
    "num_envs = 16\n",
    "env_name = \"MountainCarContinuous-v0\"\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name)\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Net\n",
    "#ActorCritic, for continuous action tasks\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_outputs),\n",
    "        )\n",
    "        #self.log_std only for continuous action spaces.\n",
    "        self.log_std = nn.Parameter(torch.ones(1, num_outputs) * std)\n",
    "        \n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        mu    = self.actor(x)\n",
    "        std   = self.log_std.exp().expand_as(mu)\n",
    "        #only difference is we're using Normal dist to sample actions from\n",
    "        #Gaussian distribution. Categorical() for discrete action space.\n",
    "        dist  = Normal(mu, std)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    \n",
    "def test_env(vis=False):\n",
    "    state = env.reset()\n",
    "    if vis: env.render()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state)\n",
    "        next_state, reward, done, _ = env.step(dist.sample().cpu().numpy()[0])\n",
    "        state = next_state\n",
    "        if vis: env.render()\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GAE\n",
    "#gamma is discount factor for returns, tau is smoothing factor of GAE algo.\n",
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        #delta is Bellman equation minus value of the state\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        #moving average of advantages discounted by gamma * tau\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
    "    batch_size = states.size(0)\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]\n",
    "        \n",
    "        \n",
    "\n",
    "def ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param=0.2):\n",
    "    for _ in range(ppo_epochs):\n",
    "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
    "            dist, value = model(state)   #pass state into network to get latest distribution and state value\n",
    "            entropy = dist.entropy().mean() #for inciting exploration\n",
    "            new_log_probs = dist.log_prob(action) #new log_probs of originally selected actions\n",
    "            \n",
    "            \n",
    "            ratio = (new_log_probs - old_log_probs).exp()\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
    "            \n",
    "            #CLIP LOSS\n",
    "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
    "            #MSE LOSS between GAE returns and estimated value of the state\n",
    "            critic_loss = (return_ - value).pow(2).mean()\n",
    "            #discounted critic loss plus CLIP LOSS minus scaled entroy\n",
    "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs  = envs.observation_space.shape[0]\n",
    "num_outputs = envs.action_space.shape[0]\n",
    "\n",
    "#Hyper params:\n",
    "hidden_size      = 256   #neurons in hidden layer\n",
    "lr               = 3e-4  #passed to Adam optimizer    \n",
    "num_steps        = 1008   #num of transitions we sample for each training iter\n",
    "mini_batch_size  = 16     #num of samples randomly selected from stored data\n",
    "ppo_epochs       = 8     #num passes over entire training data\n",
    "threshold_reward = 90    #we'll stop training when we reach this reward in evaluation\n",
    "\n",
    "model = ActorCritic(num_inputs, num_outputs, hidden_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAE/CAYAAABFK3gIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8XXWd//HXJ3ubpHu6t3ShLW0ZWjGWRTalguCCuCDgIAhOZYRRf7M4KD+VH7iMjsjIuE0dGdBhExkEkZFFFkFEKdCGLum+JSTpmq1p1vv5/XFOym1I2tskJ+fe5P18PO4j95zvufd87rk373vu92zm7oiIyOCXFXcBIiIyMBT4IiJDhAJfRGSIUOCLiAwRCnwRkSFCgS8iMkQo8HvBzOaZ2UozazCzz8Vdj0TLzLaZ2dK46xDpKwV+73wReMbdi9399riLSWZmc83sYTPbbWb7zOxxM5vXZZr/Y2bVZlZvZneYWX5S2wwze8bMmsysvGvQ9eWxQ4GZjTKzu8xsV3i7qUt7ysvIzO40s1Yza0y6ZYdtn+gyvsnM3MzeHrb/k5mtDldKtprZP/Uwj7PDx309aVy+md1mZm+Y2X4z+5GZ5Sa1P2tmzUnzXp/U9uUudR00s4SZjYvyNZnZ9C6PbQwf+w9J01xuZtvN7ICZ/drMxiS1jTGzh8K27WZ2eVKbmdmNZrYj/NzfZ2YjjvAxSF/urtsx3oCngE8foT07xtqWANcAY4Bc4BagPKn9fKAGWAiMBp4F/iWp/U/A94BhwEeAWqCkr489xteQE9Oy63a+wDZgaYrP8V/AA8BwYAawGfhUb5YRcCfw9RTne1U4LwuHvwicDOQA84DtwKVdHpMLrAReSp4P8DXg+fAzVBK2/7+k9meP9PnvMo+bgKcH6jUlPXYm0AHMCIcXAg3AWUARcA9wX9L09wL3h21nAHXAwrDtSqAcmBa2PwzcFcdntM+f8bgLyLQb8HT4QWoGGoG54Yf4x8BjwAFgKfA+4DWgHtgJ3JT0HDMABz4Vtu0HrgXeAZSFIfCDLvO9GlgXTvs4cFyK9Y4J5zU2HL4H+GZS+7lAdXh/LtACFCe1Pw9c29fHplDnNuCfw9ffEv5TTwYeBHYDW4HPhdMWAAeBceHwjUA7MCIcvgX4t/B+Ku/DNcAO4A/h+CsIwmRv+NzbSD3w9wDvSBr+MvB8b5YRxxaOzwBfO0L77cC/dxl3A/CdrvMBVgAfSxq+HNiZNPwsKQQ+YMAW4MqBek1JbV8j+BXeOfxN4J6k4dlAK1AMFIb35ya1/4JwZQb4FfBPSW2nE/z/D0/ldaTTTV06x8jd303wT3q9uxe5+4aw6XLgGwQfoBcIgv+TwCiC0PlbM/tQl6c7BZgDfBz4N4JwWUqwNnKJmZ0NYGYXEQTHhwnWuJ4nWCNJxVkEobw3HF4IrEpqXwVMMLOxYdsWd2/o0r6wHx6bissIltUoIAH8JnyOKQRfLl8ws/PdvRl4GTg7fNzZBAH9zqTh58L7qbwPZwPzgfPNbAHBl/cVBF84Y4GpnROa2RlmVnuU12Fd7p8Y3u/NMvqsBV1zr5jZR7qdmdlxBO/zz3toN+BMYE2Xx1wN3Jzia5hqZiOTxn3LzPaY2R/N7JwenuNMYDzBl3bkr6lL2yeBu5JGH/bZdffNhCEf3tqT/pfhre9L1+WRT/C/m1EU+P3nYXf/o7sn3L3Z3Z9199fD4TKCgD67y2NuCad9giCY7nX3Xe5eSRDqbwunuxb4lruvc/d2grWVxeE/RY/MbCrwQ+Dvk0YXEfxc7dR5v7ibts724n54bCpud/ed7n6Q4NdOibvf7O6t7r4F+ClwaTjtc8DZZpYDnESwtne2mRWEj/0DQIrvw03ufiCc70eBR939D+7eAnyF4MuH8PlecPdRR3gNvwNuMLNiMzueIFSHh23HuoxuJwiV8WEdd5rZO7uZ7pMEvyK29vA8NxH8r/9Xl+f+irs39vAaPm9mJWY2EejcMaHzdfwzMIvgi3g58Bszm93N81wJ/KrLPKJ8TZ3OACYQrJl3OtKyLyL4BdhdGwTL49MWbH8ZSfD64c3lkTEU+P1nZ/KAmZ1iwca53WZWRxDa47o8pibp/sFuhovC+8cB3zez2nDtch/BWsaUnooxsxLgCeBH7p78a6ARSN7g1Hm/oZu2zvbONdK+PDYVycvwOGBy52sOX/eXCf6RIQj8cwj6dF8HniQI8lOBTZ2/aFJ8H5LnOzl52N0PEHTtpOpzBO/dRoK+3nuBirDtmJaRu7/q7nvdvd3dHwPuJviV11XXtdlDzOz6sP194RcYZvYBgm6l+3t4Dd8g6AZbCbwI/BpoI/x8uvuf3b3B3Vvc/S7gj8CFXeY7HPhY17qiek1dXAk82OWL5kjL/mjvyx0E7+OzBL8ongnHV5BhFPj9p+tpR+8BHgGmuftI4Ccc/rPwWOwEPuPuo5Juw9z9xe4mNrPRBGH/iLt/o0vzGmBR0vAioCYMyDXALDMr7tK+ph8em4rkZbgT2NrlNRe7e2ewvEiw4e5i4Dl3XwtMJwie55KeJ5X3IXm+VQQb54BDwTU25Rfgvs/dP+HuE919IcH/2F/C5r4uI+9ae7h2PJnD12Y7264m6Kc/192Tw+lcoNSCva2qCboUv2BmD4ev4aC7X+/uU9x9FsEX3ivunug6j57qInhf9hGE5EC8ps72YXTzRUOXz66ZzSLoltkQ3nLMLLmL5tD7Ev46/Jq7z3D3qeH4yvCWWeLeiJCJN7pstKKbDVHALsKNVQR7zuwC/jscnkHwQc9Jmr4COCdp+L+B/xvevxhYzZt7DYwkaaNal/mOIAiYH/TQ/l6gGlhA0K/9NIfvafMS8F2CDaMXc/heOr1+bArLdBtJG0aBbOBVgp/Pw8LhEzl8g+iLBD/FzwyHHwiHkzc4Huv7sJBgje8MIC98Pe2kvtF2NsEXRDZwAcFG3IW9WUYE3UtFBF8a5xGscZ7TZZrlwM+7eewnwvdqfjdtxcDEpNv9wG3AmLB9CkHgGsEvpp3AeWHbKIK9tQoINqx/gqA7cm6XeTwB3DxQrylpmsvDz5J1Gb+w87NCsJH2vzl8L537CNbiCwm2BSXvpTMmfF+N4LO/GlgWZcZEdYu9gEy8kVrgf5RgQ2ID8Cjwg6METY+BHw5fQdB10bm3yR091HZl+NwHCIKr8zY9aZq/J/h5Xk/QB5qf1DYjfH0HgfV0CbrePjb8Z11zhGW6rZt5TQ7/CasJ9k56qctzfiucV344fH342if09n1IWoY76GYvHYLAaDzC67gEeANoIugSOb9Le8rLiGA7Tl24rFfx1t0qCwi+MM7tpo6tBN0wyZ+Bn/RQ850cvpfOWeFrbgpr/ERSWwnBBvOGcN4vAe/p8nxTCL4kj+9mXpG+JoI92G7p4XVeHr6vBwi628YktY0h6Lo6EE5zeVLb3HA5NIWfpb8fqKzp71vn/q0iIjLIqQ9fRGSI6JfAt+AQ+11mtjpp3Bgze9LMNoZ/R4fjzcxuN7NNZlZmZif3Rw0iInJk/bWGfyfBBr1kNwC/d/c5wO/DYQg2ZM0Jb8sIDnIREZGI9Uvgu/sfCHbBSnYRb+4adRfwoaTxP/fAS8AoM5vUH3WIiEjPouzDn+DuVeH9at48YGYKhx/oUsERDiASEZH+kTMQM3F3N7Nj2h3IzJYRdPlQWFj49hNOOCGS2kREMt0rr7yyx91LjjZdlIFfY2aT3L0q7LLZFY6vJOlIRoITU73liDV3X05wAAalpaW+YsWKCEsVEclcZrY9lemi7NJ5hOAAFsK/DyeN/2S4t86pQF1S14+IiESkX9bwzexeghNZjTOzCoJzUf8L8Eszu4bg6LRLwskfIzjfySaCI9c+1R81iIjIkfVL4Lv7ZT00ndvNtA5c1x/zFRGR1OlIWxGRIUKBLyIyRCjwRUSGCAW+iMgQocAXERkiFPgiIkPEgJxaQURE3qq5rYNNuxrZUNPA7JIiFk0bFen8FPgiIhFrbU+wdc8B1tc0sLGmgfXVDWyoaWDHviYS4VnG/ubMmQp8EZFM0ZFwtu89wIaaBjbUNLK+poEN1Q1s3XOA9jDZs7OMGWOHM3/SCC5aPIW5E4qZN7GI48YWRl6fAl9E+kUi4exvaqWlPUFLe4LW8NbS3hH+Dcd3JGhp66C1I3FofOd0CYcsA8PIMsDssGEzMDPMIMsMI/wbjk8knA53OhJOe0fn/QTtCSeR8MP+diTd2sPH4eA47gS3zvsEwxw27IfGO7CnoYXNuxtpaU8cWibTxwxn7oRi3rNgAvMmFjNnfDGzSgopyM2O4R1S4ItIL7S0d7CxppE1b9Sx9o161lbVs66qgcaW9l4/Z5YFa7/ukEgK077KyTKysoycLCPbjOzs4H6WheOyg/FZWcEXiFnn3+CLxix4nsPGJ7VZ0EhJcT5nzBnHnPFFzJtYzPHjixiel14Rm17ViEjaqW1qZW1VfRDsYbhv2tV4qIuiMC+bBZNH8JGTpzBzXCHD8rLJy8kiPyebvOys8H7wt3N8fk6X8dlZ5GR3v9Ogu5PwpL9Ja+CdXwwJdzwBWVmQk5VFdpaRndX5q8AGcGmlNwW+iADQ1pGguq75zXAP/1bWHjw0zYQR+SyYNIJz549n4eSRLJg0guljhpOVFV2omhnZwWp0ZPMYKhT4IoNcIuHsa2qlpr45vLVQXdfMrobD7+9pbD30GDOYNa6Qtx83mitOO44Fk0awYPIIxhXlx/hKpK8U+CIZyN2pb25n34FW9h1oYW9jK/sOtLL3QCu7G4IQr2loZld9C7sammnreGtn+LiiPCaMKGDiyAIWTRvFhBH5TBxRwLyJxcybWJx2/c/Sd3pHRQaQu9PW4bR1JGjrCPZYaetw2trfHD7Q0hGE+IFW9jUGIb4vvO1pbGHfgVb2N7V2G+IAxfk5TBhZwIQR+Zwyc0xwvzifiSMLGD+igIkjCigpzie3hz5zGbwU+CL9pK0jwd0vbee+l3dysK2DtvYErUnh3haG+7Eqzs9hTFEeYwrzmDp6GCdNHcmYwnzGFgbjxhTlMbYwj7FF+YwZnsewvHh2+ZP0p8AX6SN359n1u/n6b9eyefcB3n7caOZPGkFutpGTHeyBkptt5GZnkRvutXLYcHYWuTlvDg/LzWZMYR5jw5DPz1GAS/9Q4Iv0wYaaBm55dC3Pb9zDrHGF/OzKUt59wnjtCihpSYEv0gt7G1u47akN3PPnHRTl5/DV9y/gr089jrwc9YtL+lLgixyD1vYEd724jduf3khTawefPG0Gnz93DqML8+IuTeSoFPgiKXB3nlhbwzcfW8f2vU28a14JN75vPsePL467NJGUKfBFjmLNG3Xc8uhaXtqyjznji7jr6iWcPbck7rJEjpkCX6QHuxqaufXxDfzylZ2MGpbLLR86kcveMa3Hc76IpLtIA9/M5gH3J42aBXwVGAX8DbA7HP9ld38sylpEUtXc1sHPXtjKj57ZRGtHgmveOZO/O3cOI4flxl2aSJ9EGvjuvh5YDGBm2UAl8BDwKeA2d/9ulPMXOVb7DrTykR+/yNY9BzhvwQS+dOF8Zo6L/sIUIgNhILt0zgU2u/t27aMs6SiRcP7xgVVU7j+ofnoZlAayM/JS4N6k4evNrMzM7jCz0QNYh0i3fvbCVp4u38VX3j9fYS+D0oAEvpnlAR8EHghH/RiYTdDdUwXc2s1jlpnZCjNbsXv37q7NIv3qtR37+fbvyrngxIn89anHxV2OSCQGag3/AuBVd68BcPcad+9w9wTwU2BJ1we4+3J3L3X30pISrW1JdOqa2rj+nteYOLKAf/nISTotggxaAxX4l5HUnWNmk5LaLgZWD1AdIodxd/75wTJq6pv598vepj1xZFCLfKOtmRUC7wE+kzT6O2a2mOBi79u6tIkMmF+8tJ3franmxgvn87bp2pQkg1vkge/uB4CxXcZdEfV8RY5mdWUdX390He8+YTzXnDEz7nJEIqdDBmVIamxp5+/ufY0xhXl892OLIr0It0i60KkVZMhxd2586HW27z3AfctOY4zOdClDhNbwZch5YEUFD698g/+zdC5LZo6JuxyRAaPAl7RzoKWduqa2SJ57Q00DX31kNe88fiyffdfxkcxDJF2pS0cGVEfC2dXQzBu1B6msDf523jqH6w62kZNlfOqdM/j80rkU5ffPx/RgawfX3f0qRfk53PbxxWSr316GGAW+9Dt356Ut+9i8uzEp0JuprD1IdX0zHQk/bPoRBTlMHjWMKaOGUXrcaCaPGsbWPY389PmtPLLqDb584Xw+uGhynw+IuumRNWza3cgvrj6F8cUFfXoukUykwJd+Vd/cxpf+53V+W1YFQE6WMXFkAZNHDWPJzDFMHhXc7wz4SSMLKC7o/mCny5ZM56sPr+Hz963k3r/s4OaLTmTuhN5dYerhlZXcv2In171rNmfMGdfr1yeSyczdjz5VzEpLS33FihVxlyFHsbqyjuvueZWK/Qf5+/fM5SMnT6WkOL9PXScdCee+l3fwnd+tp7GlnU+dPoPPL53T45dEd7buOcD7b3+e+ZNGcN+yU3UBExl0zOwVdy892nRaw5c+c3d+/qftfOO36xhblMf9y06ldEb/7P2SnWV84pTjuODESfzr4+X87I9vdvNctPjo3TzNbUG/fW5OFrdf9jaFvQxp+vRLn9Q3t/HZu1/la4+s4Yw54/jt587st7BPNqYwj299+CQe+uw7mTiygC/cv5KPL3+J9dUNR3zctx5bx9qqer770UVMHjWs3+sSySQKfOm1sopa3nf78zyxtoYvX3gC//nJ0sgPYlo8bRQPffadfPPiv2JDTQMX3v48N/9mLfXNb92N83erq7jrT9v59BkzWbpgQqR1iWQCBb4cM3fnv/64lY/8+EU6OpxffuY0lp01e8BOT5CdZVx+ynSe+Ydz+Pg7pvFfL27l3Fuf46HXKujcJrVzXxP/9KsyFk0dyRffe8KA1CWS7rTRVo5JXVMbX3xwFY+vqWHp/PF892OLGDU83lMTlFXU8pVfr2ZVRR1LZozhK+9fwFceXs3mXY389nNnMn3s8FjrE4laqhttFfiSspU7a7n+nleprmvmhgtO4JozZqbNxUISCeeXK3by7d+Vsz88SveHl5/M+06adJRHimQ+7aUj/cbd+dkLW/n278oZX1zAA9eelnbnjs/KMi5dMp33njiR23+/ibFFeQp7kS4U+HJEtU2t/OMDZTy1robzFkzgXz+6iJHD0/eqUKOG5/HVDyyIuwyRtKTAlx69umM/f3fPa+xqaOZrH1jAVafPSJsuHBE5dgp8eYvW9gT/+cIWvvfEBiaNKuBX157Oommj4i5LRPpIgS+HuDtPrq3hm4+tY9veJi78q4l868Mn6cLeIoOEAl8AWFdVzy2PruXFzXs5fnwRd37qHZwzb3zcZYlIP1LgD3F7Glu49YkN3P/yDkYMy+XmixZy+ZLpOueMyCCkwB+iWto7uPOP2/j3pzfR3NbBVafP5PPnzknrPXBEpG8U+EOMu/P4mmq++Vg5O/Y1sXT+eL504XxmlxTFXZqIREyBP4SsrqzjlkfX8uet+5g3oZhfXLOEM+eUxF2WiAyQyAPfzLYBDUAH0O7upWY2BrgfmAFsAy5x9/1R1zJU7Wpo5ruPr+eBVyoYPTyPr3/oRC59xzT104sMMQO1hv8ud9+TNHwD8Ht3/xczuyEc/ucBqmXIaG7r4GcvbOVHz2yitSPBp8+YyfXvnqPdLEWGqLi6dC4Czgnv3wU8iwK/X71eUcff3v0KFfsPct6CCXzpwvnMHFcYd1kiEqOBCHwHnjAzB/7D3ZcDE9y9KmyvBnR1in62/PktNLa0c8+nT+H043XRbhEZmMA/w90rzWw88KSZlSc3uruHXwaHMbNlwDKA6dOnD0CZg0tZRS2nzhyrsBeRQyLfaufuleHfXcBDwBKgxswmAYR/d3XzuOXuXurupSUl2pPkWNQ2tbJ9bxMnTRsZdykikkYiDXwzKzSz4s77wHnAauAR4MpwsiuBh6OsY6gpq6gDYNFUnfBMRN4UdZfOBOCh8JS6OcA97v47M3sZ+KWZXQNsBy6JuI4hpayiFoATp2gNX0TeFGngu/sWYFE34/cC50Y576FsVUUds8YVavdLETmMjrwZhMoqajlpqtbuReRwCvxBpqa+mZr6Fk5S/72IdKHAH2RW7Qz67xdpDx0R6UKBP8iUVdSRnWUsmKTAF5HDKfAHmVUVtcydUMywvOy4SxGRNKPAH0Tcndcr61ikDbYi0g0F/iCyY18TtU1t2mArIt1S4A8iq8IjbLVLpoh0R4E/iJTtrCU/J4t5E4vjLkVE0pACfxApq6xjweQR5OpKViLSDSXDINGRcFZX1umEaSLSIwX+ILF5dyNNrR3qvxeRHinwB4nOI2y1h46I9ESBP0iUVdRRlJ/DLF23VkR6oMAfJMoqajlxygiysizuUkQkTSnwB4HW9gTrqhq0wVZEjkiBPwiUV9fT2pFQ/72IHJECfxDQEbYikgoF/iBQtrOWMYV5TB09LO5SRCSNKfAHgbKKOk6aOpLwYvEiIt1S4Ge4ptZ2Nu5qUP+9iByVAj/Dra6sJ+HoHPgiclQK/AxXVqEjbEUkNQr8DLeqoo7JIwsoKc6PuxQRSXORBb6ZTTOzZ8xsrZmtMbPPh+NvMrNKM1sZ3i6MqoahoKyiVmv3IpKSnAifux34B3d/1cyKgVfM7Mmw7TZ3/26E8x4Sapta2b63iY+/Y1rcpYhIBogs8N29CqgK7zeY2TpgSlTzG4rKwgOudEoFEUnFgPThm9kM4G3An8NR15tZmZndYWajB6KGwahzg+2JU7SHjogcXeSBb2ZFwIPAF9y9HvgxMBtYTPAL4NYeHrfMzFaY2Yrdu3dHXWZGWlVRx6xxhYwclht3KSKSASINfDPLJQj7u939fwDcvcbdO9w9AfwUWNLdY919ubuXuntpSUlJlGVmrGCDrdbuRSQ1Ue6lY8DPgHXu/r2k8ZOSJrsYWB1VDYNZTX0zNfUt2kNHRFIW5V467wSuAF43s5XhuC8Dl5nZYsCBbcBnIqxh0Oq8pOGiaVrDF5HURLmXzgtAd2fzeiyqeQ4lZRV1ZGcZCyYp8EUkNTrSNkOVVdYxd0Ixw/Ky4y5FRDKEAj8DuXuwwVa7Y4rIMVDgZ6Cd+w5S29TGSeq/F5FjoMDPQKvCA650hK2IHAsFfgYqq6glLyeLeROL4y5FRDKIAj8DraqoY8GkEeRm6+0TkdQpMTJMR8JZXVmnK1yJyDFT4GeYzbsbaWrt0BG2InLMFPgZRkfYikhvKfAzTFlFHUX5OcwaVxR3KSKSYRT4GaasopYTp4wgK6u7s1aIiPRMgZ9BWtsTrKtq0P73ItIrCvwMUl5dT2tHQhtsRaRXFPgZZFV4DVtd9EREekOBn0HKdtYypjCPqaOHxV2KiGQgBX4GKauo46SpIwkuJiYicmwU+BmiqbWdjbsa1H8vIr2mwM8QqyvrSTg6pYKI9JoCP0OUhadE1hq+iPSWAj9DrKqoY/LIAkqK8+MuRUQylAI/Q5RV1GrtXkT6RIGfAWqbWtm+t4m/Uv+9iPSBAj8DlIUHXOmUCiLSFwr8DNC5wVZr+CLSF7EFvpm918zWm9kmM7shrjoywaqKOmaOK2TksNy4SxGRDBZL4JtZNvBD4AJgAXCZmS2Io5ZM8Hp4hK2ISF/EtYa/BNjk7lvcvRW4D7goplrS2q76Zqrrm7WHjoj0WVyBPwXYmTRcEY6TLlYd2mCrNXwR6Zu03WhrZsvMbIWZrdi9e3fc5cSmrKKW7Cxj4WQFvoj0TVyBXwlMSxqeGo47xN2Xu3upu5eWlJQMaHHpZFVFHXPGFzEsLzvuUkQkw8UV+C8Dc8xsppnlAZcCj8RUS9pyd8oqarX/vYj0i5w4Zuru7WZ2PfA4kA3c4e5r4qglne3cd5DapjZOmqbuHBHpu1gCH8DdHwMei2v+mWBVeMCV1vBFpD+k7UZbCTbY5uVkMW9icdyliMggoMBPY6sq6lgwaQS52XqbRKTvlCRpqiPhrK6s0/73ItJvFPhpatOuRppaO3SErYj0GwV+mvrvl7aTk2WcNnts3KWIyCChwE9DFfubuO/lHXysdBqTRw2LuxwRGSQU+GnoB09vwjD+7t3Hx12KiAwiCvw0s33vAR54pYLLlmjtXkT6lwI/zXz/9xvJyTKue5fW7kWkfynw08imXY38+rVKPnnacYwfURB3OSIyyCjw08j3f7+Rgtxsrj17dtyliMggpMBPE+XV9Txa9gZXnT6DsUX5cZcjIoOQAj9N3PbkBoryclh21qy4SxGRQUqBnwZWV9bx+Joarj5jJqOG58VdjogMUgr8NPC9Jzcwclgu15w5M+5SRGQQU+DH7NUd+3m6fBfLzprFiILcuMsRkUFMgR+z257cwJjCPK46fUbcpYjIIKfAj9Fftu7j+Y17uPbsWRTmx3bxMREZIhT4MXF3bn1iPSXF+Vxx6oy4yxGRIUCBH5MXN+/lz1v38dlzZjMsLzvuckRkCFDgx6Bz7X7SyAIuWzI97nJEZIhQ4Mfg2Q27eXVHLde963gKcrV2LyIDQ4E/wNyd257cwNTRw7ikdFrc5YjIEKLAH2BPrq2hrKKOz717Dnk5WvwiMnAiSRwz+1czKzezMjN7yMxGheNnmNlBM1sZ3n4SxfzTVSLhfO/JDcwYO5wPnzwl7nJEZIiJahXzSeBEdz8J2AB8Kalts7svDm/XRjT/tPS/q6spr27gC0vnkpOttXsRGViRpI67P+Hu7eHgS8DUKOaTSToSzm1PbWDO+CI+sGhy3OWIyBA0EKuZVwP/mzQ808xeM7PnzOzMAZh/WvjNqjfYtKuRLyydS3aWxV2OiAxBvT6e38yeAiZ203Sjuz8cTnMj0A7cHbZVAdPdfa+ZvR34tZktdPf6bp5/GbAMYPr0zN5Xvb0jwfd/v5ETJhZzwYndLTIRkej1OvDdfemR2s3sKuD9wLnu7uFjWoCW8P4rZrYZmAus6Ob5lwPLAUpLS723daaD/3mtkq17DrD8ireTpbV7EYlJVHvpvBf4IvBBd29KGl9iZtnh/VnAHGBLFDUJaJMWAAAO6klEQVSki9b2BN9/aiMnTR3JexZMiLscERnCojpF4w+AfOBJMwN4Kdwj5yzgZjNrAxLAte6+L6Ia0sIvV+yksvYgX7/4RMJlISISi0gC392P72H8g8CDUcwzHTW3dfCDpzdx8vRRnDO3JO5yRGSI087gEbr3Lzuorm/mH86bp7V7EYmdAj8i7s5PntvMKTPHcPrssXGXIyKiwI/KG3XN1NS38IFFk7V2LyJpQYEfkfKq4NCC+ZOKY65ERCSgwI9IeXUDAHMnKPBFJD0o8COyrqqeqaOHUVyQG3cpIiKAAj8y5dUNnDBxRNxliIgcosCPQHNbB1v3HFD/vYikFQV+BDbtaqQj4VrDF5G0osCPQOcG2xO0hi8iaUSBH4Hyqnryc7KYMbYw7lJERA5R4EegvLqBeROLdaETEUkrCvwIlFfXM0/734tImlHg97PdDS3saWzlhEnaYCsi6UWB38/Whxts50/UGr6IpBcFfj8rrw7OoTNPgS8iaUaB38/WVTUwvjifsUX5cZciInIYBX4/K6+uV/+9iKQlBX4/au9IsLGmkRPUnSMiaUiB34+27jlAa0dCgS8iaUmB348OnVJB59ARkTSkwO9H5dX15GQZs8frlAoikn4U+P2ovKqB2SVF5Odkx12KiMhbKPD7UXl1g86QKSJpS4HfT+oOtlFZe1D99yKStiILfDO7ycwqzWxleLswqe1LZrbJzNab2flR1TCQ1h/aYKs1fBFJTzkRP/9t7v7d5BFmtgC4FFgITAaeMrO57t4RcS2RWh+eUkFdOiKSruLo0rkIuM/dW9x9K7AJWBJDHf1qXXUDI4flMnFEQdyliIh0K+rAv97MyszsDjMbHY6bAuxMmqYiHHcYM1tmZivMbMXu3bsjLrPvyqvqOWFiMWa66ImIpKc+Bb6ZPWVmq7u5XQT8GJgNLAaqgFuP5bndfbm7l7p7aUlJSV/KjFwi4ayvbmC+zqEjImmsT3347r40lenM7KfAo+FgJTAtqXlqOC5jVew/yIHWDm2wFZG0FuVeOpOSBi8GVof3HwEuNbN8M5sJzAH+ElUdA2HdoQ22WsMXkfQV5V463zGzxYAD24DPALj7GjP7JbAWaAeuy/Q9dMqrGjCDuROK4i5FRKRHkQW+u19xhLZvAN+Iat4DbX1NPceNGc7wvKj3chUR6T0dadsPyqsadIStiKQ9BX4fHWztYOveAzrgSkTSngK/jzbUNOCuc+CLSPpT4PdRebiHznyt4YtImlPg99G6qgaG52UzbfTwuEsRETkiBX4fra9uYO6EYrKydEoFEUlvCvw+cHfKq+vVnSMiGUGB3we7GlrY39SmDbYikhEU+H2wrio8pYLOoSMiGUCB3wflh65ypTV8EUl/Cvw+KK+qZ/LIAkYOz427FBGRo1Lg90F5dQPz1J0jIhlCgd9Lre0JNu9u1CmRRSRjKPB7acueRto6XBtsRSRjKPB7qbwq2GCryxqKSKZQ4PfSuup68rKzmDmuMO5SRERSosDvpfKqBo4fX0RuthahiGQGpVUvlVfX6xz4IpJRFPi9sP9AKzX1LdpgKyIZRYHfCzrCVkQykQK/FzoveqIuHRHJJAr8XiivamBsYR4lRflxlyIikjIFfi90brA100VPRCRzKPCPUUfCWV/ToP57Eck4OVE8qZndD8wLB0cBte6+2MxmAOuA9WHbS+5+bRQ1RGXHviaa2xI6aZqIZJxIAt/dP95538xuBeqSmje7++Io5jsQysOLnszXGr6IZJhIAr+TBZ3clwDvjnI+A2lddQNZBnMmFMVdiojIMYm6D/9MoMbdNyaNm2lmr5nZc2Z2Zk8PNLNlZrbCzFbs3r074jJTV15Vz8xxhRTkZsddiojIMen1Gr6ZPQVM7KbpRnd/OLx/GXBvUlsVMN3d95rZ24Ffm9lCd6/v+iTuvhxYDlBaWuq9rbO/lVc38FdTR8ZdhojIMet14Lv70iO1m1kO8GHg7UmPaQFawvuvmNlmYC6word1DKTGlnZ27GviktKpcZciInLMouzSWQqUu3tF5wgzKzGz7PD+LGAOsCXCGvrVhhqdUkFEMleUG20v5fDuHICzgJvNrA1IANe6+74Ia+hXnRc90S6ZIpKJIgt8d7+qm3EPAg9GNc+olVfXU5Sfw9TRw+IuRUTkmOlI22NQXtXACRN1SgURyUwK/BS5O+t00RMRyWAK/BS9UddMQ3O7NtiKSMZS4Kfo0CkVtIYvIhlKgZ+izqtczZ2gwBeRzKTAT1F5dQNTRw+juCA37lJERHpFgZ+i8qp69d+LSEZT4Kegua2DLXsOqP9eRDKaAj8Fm3Y10pFwreGLSEZT4Kegc4Ot9sEXkUymwE/B+up68nOymDG2MO5SRER6TYGfgvLqBuZOKCY7S6dUEJHMpcBPwbrwHDoiIplMgX8Uuxta2NPYwgmTtMFWRDKbAv8o1ocbbOdrDV9EMpwC/yjKq4Nz6OiiJyKS6RT4R1Fe3cD44nzGFuXHXYqISJ8M6sCvrD3Ib1a9wZ7Gll4/R3l1vfrvRWRQiPKatrF7ck01N/1mLQDzJhRz2uyxnD57LKfMGsvIYUc/CVp7R4INNY1cdfq4qEsVEYncoA78vz71OBZPH82Lm/fwp817ue/lHdz54jayDE6cMjL8AhjHO2aMZnjeWxfFtr0HaG1PaJdMERkUBnXg52RnsXjaKBZPG8VnzzmelvYOVu6o5cXNe/nT5r3c8cJW/uO5LeRkGYunjeL02WM5bfY43jZ9FAW52ayrCk+poHPoiMggYO4edw1HVVpa6itWrOj3521qbeeV7ft5cfNeXty8l9crakk45OdkUTpjNG3tzqs79rPm5vPJz8nu9/mLiPQHM3vF3UuPNt2gXsM/muF5OZw5p4Qz55QAUN/cxl+27At+AWzZy7qqehZNG6WwF5FBYUgHflcjCnJZumACSxdMAGDfgVZysnX+HBEZHPq0W6aZfczM1phZwsxKu7R9ycw2mdl6Mzs/afx7w3GbzOyGvsw/amMK8xihSxqKyCDR1/3wVwMfBv6QPNLMFgCXAguB9wI/MrNsM8sGfghcACwALgunFRGRiPWpS8fd1wGYvaXb4yLgPndvAbaa2SZgSdi2yd23hI+7L5x2bV/qEBGRo4vqSNspwM6k4YpwXE/jRUQkYkddwzezp4CJ3TTd6O4P939Jh+a7DFgGMH369KhmIyIyZBw18N19aS+etxKYljQ8NRzHEcZ3ne9yYDkE++H3ogYREUkSVZfOI8ClZpZvZjOBOcBfgJeBOWY208zyCDbsPhJRDSIikqRPG23N7GLg34ES4LdmttLdz3f3NWb2S4KNse3Ade7eET7meuBxIBu4w93X9OkViIhISob0qRVERAaDVE+tMKjPhy8iIm9S4IuIDBEZ0aVjZruB7b18+DhgTz+W09/SuT7V1jvpXBukd32qrXeOc/eSo02UEYHfF2a2IpW+rbikc32qrXfSuTZI7/pUW7TUpSMiMkQo8EVEhoihEPjL4y7gKNK5PtXWO+lcG6R3faotQoO+D19ERAJDYQ1fREQYRIF/tCtphef1uT9s/7OZzRiguqaZ2TNmtja8Otjnu5nmHDOrM7OV4e2rA1Fb0vy3mdnr4bzfckizBW4Pl12ZmZ08QHXNS1omK82s3sy+0GWaAVt2ZnaHme0ys9VJ48aY2ZNmtjH8O7qHx14ZTrPRzK4cwPr+1czKw/ftITMb1cNjj/gZiKi2m8ysMum9u7CHx0Z6lbwears/qa5tZrayh8dGutz6nbtn/I3gvDybgVlAHrAKWNBlms8CPwnvXwrcP0C1TQJODu8XAxu6qe0c4NEYl982YNwR2i8E/hcw4FTgzzG9x9UE+xvHsuyAs4CTgdVJ474D3BDevwH4djePGwNsCf+ODu+PHqD6zgNywvvf7q6+VD4DEdV2E/CPKbzvR/zfjqK2Lu23Al+NY7n1922wrOEvIbySlru3Ap1X0kp2EXBXeP9XwLnWzaW6+pu7V7n7q+H9BmAdmXfRl4uAn3vgJWCUmU0a4BrOBTa7e28PwOszd/8DsK/L6OTP1V3Ah7p56PnAk+6+z933A08SXPoz8vrc/Ql3bw8HXyI4JfmA62HZpSKV/+3Iagsz4hLg3v6cZ1wGS+CnciWtQ9OE/wB1wNgBqS4UdiO9DfhzN82nmdkqM/tfM1s4kHUBDjxhZq+EF57pKh2uVHYpPf/TxbnsJrh7VXi/GpjQzTTpsPwArib4pdado30GonJ92N10Rw/dYXEvuzOBGnff2EN7XMutVwZL4Kc9MysCHgS+4O71XZpfJeiqWERwuulfD3B5Z7j7yQQXl7/OzM4a4PkfUXjthA8CD3TTHPeyO8SD3/hpudubmd1IcKryu3uYJI7PwI+B2cBioIqg6yTdXMaR1+7T+n+nq8ES+Ee6wtZbpjGzHGAksHcgijOzXIKwv9vd/6dru7vXu3tjeP8xINfMxg1EbeE8K8O/u4CHePOC851SWb5RugB41d1rujbEveyAms7urfDvrm6miXX5mdlVwPuBT4RfSm+Rwmeg37l7jbt3uHsC+GkP84xt2YU58WHg/p6miWO59cVgCfxUrqT1CNC5d8RHgad7+vD3p7AP8GfAOnf/Xg/TTOzcnmBmSwjel4H6Mio0s+LO+wQb+VZ3mewR4JPh3jqnAnVJ3RgDoce1rDiXXSj5c3Ul0N11nh8HzjOz0WG3xXnhuMiZ2XuBLwIfdPemHqZJ5TMQRW3J24Eu7mGecV4lbylQ7u4V3TXGtdz6JO6txv11I9iTZAPBFv0bw3E3E3zQAQoIugQ2EVxucdYA1XUGwc/8MmBleLsQuBa4NpzmemANwR4ILwGnD+BymxXOd1VYQ+eyS67PgB+Gy/Z1oHQA6yskCPCRSeNiWXYEXzpVQBtBX/I1BNuBfg9sBJ4CxoTTlgL/mfTYq8PP3ibgUwNY3yaCPvDOz17nnmqTgceO9BkYgNp+EX6eyghCfFLX2sLht/xvR11bOP7Ozs9Z0rQDutz6+6YjbUVEhojB0qUjIiJHocAXERkiFPgiIkOEAl9EZIhQ4IuIDBEKfBGRIUKBLyIyRCjwRUSGiP8PaOCDVPIRmi4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_frames = 50000\n",
    "frame_idx  = 0\n",
    "test_rewards = []\n",
    "\n",
    "state = envs.reset()\n",
    "early_stop = False\n",
    "\n",
    "while frame_idx < max_frames and not early_stop:\n",
    "\n",
    "    log_probs = []\n",
    "    values    = []\n",
    "    states    = []\n",
    "    actions   = []\n",
    "    rewards   = []\n",
    "    masks     = []\n",
    "    entropy = 0\n",
    "    \n",
    "    #each step generate state, action, reward, next_state from each env.\n",
    "    for _ in range(num_steps):\n",
    "        state = torch.FloatTensor(state).to(device) \n",
    "        dist, value = model(state) #state through netwwork to get prob dist and estimated V(s)\n",
    "\n",
    "        action = dist.sample()\n",
    "        #state, reward, done is list of results, 1 per env\n",
    "        #env.render()\n",
    "        next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
    "\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy += dist.entropy().mean()\n",
    "        \n",
    "        #Store log_probs, values, rewards, done_masks, states, actions. Each list num_steps long, each step num_envs wide.\n",
    "        log_probs.append(log_prob) #\n",
    "        values.append(value)\n",
    "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        \n",
    "        state = next_state\n",
    "        frame_idx += 1\n",
    "        \n",
    "        if frame_idx % 1000 == 0:\n",
    "            test_reward = np.mean([test_env() for _ in range(10)])\n",
    "            test_rewards.append(test_reward)\n",
    "            plot(frame_idx, test_rewards)\n",
    "            if test_reward > threshold_reward: early_stop = True\n",
    "            \n",
    "    #to calc returns correctly, run final next_state through network to get value\n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    _, next_value = model(next_state)\n",
    "    #run GAE. Loop backwards from recent experience.\n",
    "    returns = compute_gae(next_value, rewards, masks, values)\n",
    "    \n",
    "    #concatanate each list inside a torch tensor.\n",
    "    #list that was num_steps long, num_envs wide becomes num_steps*num_envs long\n",
    "    returns   = torch.cat(returns).detach()\n",
    "    log_probs = torch.cat(log_probs).detach()\n",
    "    values    = torch.cat(values).detach()\n",
    "    states    = torch.cat(states)\n",
    "    actions   = torch.cat(actions)\n",
    "    advantage = returns - values\n",
    "    \n",
    "    ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 reward: 90.04411398481876\n",
      "episode: 1 reward: 93.06920536448789\n",
      "episode: 2 reward: 90.79883406043746\n",
      "episode: 3 reward: 91.36870488536934\n",
      "episode: 4 reward: 92.71128692953972\n",
      "episode: 5 reward: 83.74733965706129\n",
      "episode: 6 reward: 87.6117949551651\n",
      "episode: 7 reward: 83.14957047055654\n",
      "episode: 8 reward: 86.39670242013085\n",
      "episode: 9 reward: 88.52277037669674\n",
      "episode: 10 reward: 90.93365676879836\n",
      "episode: 11 reward: 84.98309142354617\n",
      "episode: 12 reward: 90.83566587855279\n",
      "episode: 13 reward: 91.91599252374073\n",
      "episode: 14 reward: 87.83173314987917\n",
      "episode: 15 reward: 91.9018999287841\n",
      "episode: 16 reward: 90.74912745382636\n",
      "episode: 17 reward: 91.62457922101427\n",
      "episode: 18 reward: 82.86205802974693\n",
      "episode: 19 reward: 84.4964597180684\n",
      "episode: 20 reward: 92.04721099930337\n",
      "episode: 21 reward: 88.64857825477138\n",
      "episode: 22 reward: 88.64656430886696\n",
      "episode: 23 reward: 90.99729522034828\n",
      "episode: 24 reward: 91.93050816894714\n",
      "episode: 25 reward: 91.25474909448135\n",
      "episode: 26 reward: 92.3058370567503\n",
      "episode: 27 reward: 88.7811225175754\n",
      "episode: 28 reward: 91.4591644039234\n",
      "episode: 29 reward: 90.6517944858711\n",
      "episode: 30 reward: 88.2064704113858\n",
      "episode: 31 reward: 92.97664811626419\n",
      "episode: 32 reward: 86.4240043232184\n",
      "episode: 33 reward: 87.23110107987047\n",
      "episode: 34 reward: 88.23255500343465\n",
      "episode: 35 reward: 89.01732177085086\n",
      "episode: 36 reward: 85.92795852046262\n",
      "episode: 37 reward: 83.26852314351308\n",
      "episode: 38 reward: 90.72227548267848\n",
      "episode: 39 reward: 88.15446991556058\n",
      "episode: 40 reward: 86.95885621779912\n",
      "episode: 41 reward: 86.66652891210283\n",
      "episode: 42 reward: 92.30505423687481\n",
      "episode: 43 reward: 88.42522184841091\n",
      "episode: 44 reward: 86.47780474819547\n",
      "episode: 45 reward: 87.8443632903697\n",
      "episode: 46 reward: 88.11521018966194\n",
      "episode: 47 reward: 90.85759772129569\n",
      "episode: 48 reward: 93.27481969274334\n",
      "episode: 49 reward: 92.58576123165616\n",
      "episode: 50 reward: 88.98683033365549\n",
      "episode: 51 reward: 87.91333394255768\n",
      "episode: 52 reward: 88.5447952983673\n",
      "episode: 53 reward: 93.52687006989845\n",
      "episode: 54 reward: 83.1105019669192\n",
      "episode: 55 reward: 91.29073614077137\n",
      "episode: 56 reward: 91.77798217285338\n",
      "episode: 57 reward: 87.55798974855449\n",
      "episode: 58 reward: 89.89465664062178\n",
      "episode: 59 reward: 88.77834716418599\n",
      "episode: 60 reward: 90.63563453759346\n",
      "episode: 61 reward: 93.5432091370091\n",
      "episode: 62 reward: 93.19224333275034\n",
      "episode: 63 reward: 88.75746536466667\n",
      "episode: 64 reward: 84.26994090726548\n",
      "episode: 65 reward: 93.02079599744405\n",
      "episode: 66 reward: 92.39588623941714\n",
      "episode: 67 reward: 87.33794651273482\n",
      "episode: 68 reward: 88.22660012951256\n",
      "episode: 69 reward: 89.97711185385774\n",
      "\n",
      "(10073, 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Save trajectories for GAIL\n",
    "from itertools import count\n",
    "\n",
    "max_expert_num = 10000\n",
    "num_steps = 0\n",
    "expert_traj = []\n",
    "\n",
    "for i_episode in count():\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state)\n",
    "        action = dist.sample().cpu().numpy()[0]\n",
    "        #Show\n",
    "        \n",
    "        #Take action\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        #show_state(env.env, step=0, info=\"\")\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        expert_traj.append(np.hstack([state, action]))\n",
    "        num_steps += 1\n",
    "    \n",
    "    print(\"episode:\", i_episode, \"reward:\", total_reward)\n",
    "    \n",
    "    if num_steps >= max_expert_num:\n",
    "        break\n",
    "\n",
    "for _ in range(10):\n",
    "    test_env(True)        \n",
    "    time.sleep(2)\n",
    "\n",
    "expert_traj = np.stack(expert_traj)\n",
    "print()\n",
    "print(expert_traj.shape)\n",
    "print()\n",
    "np.save(\"expert_traj_mntcarcont16.npy\", expert_traj)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (env._spec.id,step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
